class My_Model(nn.Module):
    def __init__(self, input_dim):
        super(My_Model, self).__init__()
        # TODO: modify model's structure, be aware of dimensions.
        self.layers = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.LeakyReLU(),
            nn.Linear(64, 128),
            nn.LeakyReLU(),
            nn.Linear(128, 64),
            nn.LeakyReLU(),
            nn.Linear(64, 16),
            nn.LeakyReLU(),
            nn.Linear(16, 1),
        )

config = {
        'seed': 101,      # Your seed number, you can pick your lucky number. :)
        'select_all': False,   # Whether to use all features.
        'valid_ratio': 0.1,   # validation_size = train_size * valid_ratio
        'n_epochs': 5000,     # Number of epochs.
        'batch_size': 256,
        'learning_rate': 1e-4,
        'early_stop': 600,    # If model has not improved for this many consecutive epochs, stop training.
        'save_path': f'./models/model_222_81_0.586010780185461.ckpt'  # Your model will be saved here.
    }

optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], amsgrad=True, weight_decay=0.05)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.8)

            for param in model.parameters():
                regulation_loss = torch.sum(torch.abs(param)**2)
            loss = criterion(pred, y)+0.001*regulation_loss