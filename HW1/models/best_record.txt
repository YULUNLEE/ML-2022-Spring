config = {
        'seed': 101,      # Your seed number, you can pick your lucky number. :)
        'select_all': False,   # Whether to use all features.
        'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio
        'n_epochs': 3000,     # Number of epochs.
        'batch_size': 256,
        'learning_rate': 1e-4,
        'early_stop': 10000000,    # If model has not improved for this many consecutive epochs, stop training.
        'save_path': f'./models/model81_0.8277163455883662.ckpt'  # Your model will be saved here.
    }

optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=0.05)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.8)

feat_idx = [38,39,40,41,42,46,47,48,53,54,55,56,57,58,62,63,64,69,70,71,72,73,74,78,79,80,85,86,87,88,89,90,94,95,96,101,102,103,104,105,106,110,111,112]
        for i in range(1,38):
            feat_idx.append(i)

        print(feat_idx)

class My_Model(nn.Module):
    def __init__(self, input_dim):
        super(My_Model, self).__init__()
        # TODO: modify model's structure, be aware of dimensions.
        self.layers = nn.Sequential(
            nn.Linear(input_dim, 32),
            # nn.Dropout(0.5),
            nn.ReLU(),
            # nn.LayerNorm(64),
            nn.Linear(32, 64),
            # nn.Dropout(0.5),
            nn.ReLU(),
            # nn.LayerNorm(32),
            nn.Linear(64, 32),
            # nn.Dropout(0.5),
            # nn.LayerNorm(8),
            nn.ReLU(),
            # nn.LayerNorm(16),
            nn.Linear(32, 16),
            nn.ReLU(),
            # nn.LayerNorm(8),
            nn.Linear(16,1)
        )